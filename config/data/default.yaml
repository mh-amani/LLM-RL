name: ???
tokenizer: null
train_files: ???
val_files: ???
prompt_key: prompt

train_dataset_type: adaptive # base or adaptive
train_size: null # null or a number if you want to use a subset of the train set
test_size: null # null or a number if you want to use a subset of the test set

curriculum_config:
  seperator: null # null, char, "\n", ' ', '=', etc
  zero_prob: 0
  reward_threshold: 0.5
  min_ratio: 0.0
  max_ratio: 0.98

sampler:
  _target_: src.utils.curriculum_sampler.CurriculumSampler

# reward_fn_key: data_source
max_prompt_length: ???
max_response_length: ???
train_batch_size: ???
val_batch_size: null # DEPRECATED: Validation datasets are sent to inference engines as a whole batch, which will schedule the memory themselves
return_raw_input_ids: False  # This should be set to true when the tokenizer between policy and rm differs
return_raw_chat: False
shuffle: True
filter_overlong_prompts: True # for large-scale dataset, filtering overlong prompts could be timeconsuming. You cat set the filter_overlong_prompts_workers to use multiprocessing to speed up.
filter_overlong_prompts_workers: 1
truncation: error
image_key: images