

python3 src/train.py experiment=grpo data=gsm8k \
    model_path="meta-llama/Llama-2-7b-chat-hf" \
    data.train_dataset_type=adaptive data.sampler=null \
    trainer.n_gpus_per_node=4 actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \
    actor_rollout_ref.actor.ppo_mini_batch_size=128 actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=4 \
    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 actor_rollout_ref.actor.optim.lr=1e-6 critic.optim.lr=1e-5 \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.8 trainer.test_freq=100 \
    trainer.total_epochs=10000 actor_rollout_ref.actor.ppo_epochs=1 data.train_batch_size=128 

### END OF COMMAND ###
